UserId: 364757227833131011
Username: (user)
Time: 2025-07-29T09:02:58.867Z

I'm dealing with some poor contributions in a project from someone generating everything w/ AI and was thinking about how much of my own learning-to-code journey involved jumping forward in near-blindness, and only finding out the mistakes later, and only realizing the actual learnings much *much* later as a compound/aggregate of having to spend thousands of hours debugging myself out of holes i coded myself into. There's a super interesting phenomena in coding where with enough effort and stubbornness, with or without AI, you can get things to "work" and not understand why. Feels incredible to get something working! But you don't *deeply* understand *why* something is working, and you don't see what else might have stopped working (until later). Before AI you'd have to go debug the actual problem (over and over and over), which is where the learning happens. Now you just keep telling the AI to fix it until it "works". Before AI, as you spent many years coding (often with others) your sense of code-as-systems grows, as does your visibility and honesty about the trade-offs you're making, and your personal blind spots. AI amplifies the false confidence part of the learning journey. Now you can get a lot "further" without the hard-earned learnings along the way, kinda like the coyote who's run off the cliff without noticing until it's too late.