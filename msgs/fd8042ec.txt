But vLLM is exiting as it allows unparalleled concurrency/parallelism for inference requests